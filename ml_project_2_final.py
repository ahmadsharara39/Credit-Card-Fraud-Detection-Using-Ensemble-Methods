# -- coding: utf-8 --
"""ML_Project_2_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F9yOAJ3OHAxwhtTs9-AyP1uzedHCIlUI
"""

import pandas as pd

# Load dataset
df = pd.read_csv('creditcard.csv')

# Display first few rows
print(df.head())

# Get dataset information
df.info()

# Check for missing values
print(df.isnull().sum())

# Remove missing values
df.dropna(inplace=True)

# Confirm missing values removal
print(df.isnull().sum())

# Check for duplicate rows
print(df.duplicated().sum())

# Remove duplicate rows
df.drop_duplicates(inplace=True)

# Confirm duplicate removal
print(df.duplicated().sum())

import matplotlib.pyplot as plt

# Plot class distribution
class_counts = df['Class'].value_counts()
print(class_counts)

plt.figure(figsize=(6, 4))
class_counts.plot(kind='bar', color=['blue', 'red'])
plt.xticks(ticks=[0, 1], labels=['Genuine (0)', 'Fraud (1)'], rotation=0)
plt.ylabel("Count")
plt.title("Class Distribution")
plt.show()

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Separate features and target
X = df.drop(columns=['Class'])
y = df['Class']

# Split data to prevent data leakage
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE on training set
smote = SMOTE(sampling_strategy=1.0, random_state=42, k_neighbors=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Print class distribution before and after SMOTE
print("Before SMOTE:")
print(y_train.value_counts())

print("\nAfter SMOTE:")
print(pd.Series(y_train_resampled).value_counts())

# Save the resampled dataset
resampled_df = pd.DataFrame(X_train_resampled, columns=X.columns)
resampled_df['Class'] = y_train_resampled
resampled_df.to_csv("smote_creditcard_fraud.csv", index=False)
print("\nResampled dataset saved as 'smote_creditcard_fraud.csv'.")

# Set display options and print statistics
pd.set_option('display.max_columns', None)
print(df.describe())

# Remove 'Time' column from training and test sets
X_train = X_train.drop('Time', axis=1)
X_test = X_test.drop('Time', axis=1)

import seaborn as sns

# Plot distribution of transaction amounts
plt.figure(figsize=(10, 6))
sns.histplot(df['Amount'], kde=True, color='blue', bins=50)
plt.title('Distribution of Transaction Amounts', fontsize=16)
plt.xlabel('Amount', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.show()

# Plot boxplot of 'Amount'
sns.boxplot(x=df['Amount'])
plt.show()

from sklearn.preprocessing import RobustScaler

# Scale the 'Amount' column in training set
scaler = RobustScaler()
X_train['Amount'] = scaler.fit_transform(X_train[['Amount']])
X_test['Amount'] = scaler.transform(X_test[['Amount']])

# Confirm scaling
print(X_train[['Amount']].head())
print(X_test[['Amount']].head())

print(X_train.head())
pd.set_option('display.max_columns', None)
print(X_train.describe())

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Train Logistic Regression
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)

# Make predictions
rf_predictions = rf_model.predict(X_test)
lr_predictions = lr_model.predict(X_test)

# Evaluate Random Forest
rf_accuracy = accuracy_score(y_test, rf_predictions)
rf_precision = precision_score(y_test, rf_predictions)
rf_recall = recall_score(y_test, rf_predictions)
rf_f1 = f1_score(y_test, rf_predictions)
rf_conf_matrix = confusion_matrix(y_test, rf_predictions)

# Evaluate Logistic Regression
lr_accuracy = accuracy_score(y_test, lr_predictions)
lr_precision = precision_score(y_test, lr_predictions)
lr_recall = recall_score(y_test, lr_predictions)
lr_f1 = f1_score(y_test, lr_predictions)
lr_conf_matrix = confusion_matrix(y_test, lr_predictions)

# Print Random Forest metrics
print("Random Forest Metrics:")
print(f"Accuracy: {rf_accuracy:.4f}")
print(f"Precision: {rf_precision:.4f}")
print(f"Recall: {rf_recall:.4f}")
print(f"F1 Score: {rf_f1:.4f}")

# Print Logistic Regression metrics
print("\nLogistic Regression Metrics:")
print(f"Accuracy: {lr_accuracy:.4f}")
print(f"Precision: {lr_precision:.4f}")
print(f"Recall: {lr_recall:.4f}")
print(f"F1 Score: {lr_f1:.4f}")

# Plot confusion matrices for Random Forest and Logistic Regression
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title("Random Forest Confusion Matrix")
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('True')

sns.heatmap(lr_conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title("Logistic Regression Confusion Matrix")
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('True')

plt.tight_layout()
plt.show()

from sklearn.naive_bayes import GaussianNB

# Train Naive Bayes Classifier
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Make predictions
nb_predictions = nb_model.predict(X_test)

# Evaluate Naive Bayes
nb_accuracy = accuracy_score(y_test, nb_predictions)
nb_precision = precision_score(y_test, nb_predictions)
nb_recall = recall_score(y_test, nb_predictions)
nb_f1 = f1_score(y_test, nb_predictions)
nb_conf_matrix = confusion_matrix(y_test, nb_predictions)

# Print Naive Bayes metrics
print("\nNaive Bayes Metrics:")
print(f"Accuracy: {nb_accuracy:.4f}")
print(f"Precision: {nb_precision:.4f}")
print(f"Recall: {nb_recall:.4f}")
print(f"F1 Score: {nb_f1:.4f}")

# Plot Naive Bayes confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(nb_conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("Naive Bayes Confusion Matrix")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from sklearn.neural_network import MLPClassifier

# Train MLP Classifier
mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
mlp_model.fit(X_train, y_train)

# Make predictions
mlp_predictions = mlp_model.predict(X_test)

# Evaluate MLP
mlp_accuracy = accuracy_score(y_test, mlp_predictions)
mlp_precision = precision_score(y_test, mlp_predictions)
mlp_recall = recall_score(y_test, mlp_predictions)
mlp_f1 = f1_score(y_test, mlp_predictions)
mlp_conf_matrix = confusion_matrix(y_test, mlp_predictions)

# Print MLP metrics
print("\nMLP Metrics:")
print(f"Accuracy: {mlp_accuracy:.4f}")
print(f"Precision: {mlp_precision:.4f}")
print(f"Recall: {mlp_recall:.4f}")
print(f"F1 Score: {mlp_f1:.4f}")

# Plot MLP confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(mlp_conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("MLP Confusion Matrix")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Stacking to improve models
from sklearn.ensemble import StackingClassifier

# Define base models for stacking
base_models = [
    ('rf', rf_model),
    ('nb', nb_model),
    ('mlp', mlp_model)
]

# Meta-learner
meta_learner = LogisticRegression()

# Create stacking classifier
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_learner)
stacking_model.fit(X_train, y_train)

# Make predictions with stacking
stacking_predictions = stacking_model.predict(X_test)

# Evaluate stacking model
stacking_accuracy = accuracy_score(y_test, stacking_predictions)
stacking_precision = precision_score(y_test, stacking_predictions)
stacking_recall = recall_score(y_test, stacking_predictions)
stacking_f1 = f1_score(y_test, stacking_predictions)
stacking_conf_matrix = confusion_matrix(y_test, stacking_predictions)

# Print stacking metrics
print("\nStacking Metrics:")
print(f"Accuracy: {stacking_accuracy:.4f}")
print(f"Precision: {stacking_precision:.4f}")
print(f"Recall: {stacking_recall:.4f}")
print(f"F1 Score: {stacking_f1:.4f}")

# Plot stacking confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(stacking_conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("Stacking Confusion Matrix")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

import numpy as np

# Bar graph for accuracy before and after stacking
model_names = ['Random Forest', 'Logistic Regression', 'Naive Bayes', 'MLP', 'Stacking']
accuracies_before = [rf_accuracy, lr_accuracy, nb_accuracy, mlp_accuracy, 0]  # 0 for stacking before
accuracies_after = [0, 0, 0, 0, stacking_accuracy]  # 0 for before stacking for others

width = 0.35  # Bar width
fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(model_names))

rects1 = ax.bar(x - width/2, accuracies_before, width, label='Before Stacking')
rects2 = ax.bar(x + width/2, accuracies_after, width, label='After Stacking')

ax.set_ylabel('Accuracy')
ax.set_title('Model Accuracy Before and After Stacking')
ax.set_xticks(x)
ax.set_xticklabels(model_names)
ax.legend()

# Label bars with their heights
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(round(height, 4)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
fig.tight_layout()
plt.show()

# Bar graph for recall before and after stacking
recalls_before = [rf_recall, lr_recall, nb_recall, mlp_recall, 0]  # 0 for stacking before
recalls_after = [0, 0, 0, 0, stacking_recall]  # 0 for before stacking for others

fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(model_names))

rects1 = ax.bar(x - width/2, recalls_before, width, label='Before Stacking')
rects2 = ax.bar(x + width/2, recalls_after, width, label='After Stacking')

ax.set_ylabel('Recall')
ax.set_title('Model Recall Before and After Stacking')
ax.set_xticks(x)
ax.set_xticklabels(model_names)
ax.legend()

autolabel(rects1)
autolabel(rects2)
fig.tight_layout()
plt.show()